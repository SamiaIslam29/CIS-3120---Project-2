# -*- coding: utf-8 -*-
"""Project #2 - Part A Question #1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZIu5M3Kcb1IBUg5_Qju1_-jYCHjtXM0q

Question #1
"""

# Question 1: Classification - Predicting Heart Disease

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, accuracy_score

# Step 1: Load the dataset
data = pd.read_csv('HeartDisease.csv')

# Step 2: Visualize the data
# 2.1 Distribution of age
plt.figure(figsize=(8, 5))
sns.histplot(data['age'], bins=20, kde=True)
plt.title('Age Distribution')
plt.xlabel('Age')
plt.ylabel('Count')
plt.show()

# 2.2 Correlation heatmap
plt.figure(figsize=(10, 8))
# Select only numeric columns for correlation calculation
correlation_matrix = data.select_dtypes(include=np.number).corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Feature Correlation Heatmap')
plt.show()

# 2.3 Boxplot for cholesterol vs heart disease
plt.figure(figsize=(8, 5))
sns.boxplot(x='target', y='cholestoral', data=data)
plt.title('Cholestoral Levels by Heart Disease Presence')
plt.xlabel('Heart Disease (0 = No, 1 = Yes)')
plt.ylabel('Cholestoral')
plt.show()

# Step 3: Split the data into training and testing sets
X = data.drop('target', axis=1)
y = data['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

## Step 4: Standardize Features
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer

# Identify categorical and numerical columns
categorical_columns = X.select_dtypes(include=['object']).columns
numerical_columns = X.select_dtypes(include=[np.number]).columns

# Define preprocessors
categorical_preprocessor = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

numerical_preprocessor = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

# Combine preprocessors into a ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_preprocessor, numerical_columns),
        ('cat', categorical_preprocessor, categorical_columns)
    ]
)

# Apply preprocessing to the training and testing data
# Keep X_train and X_test as DataFrames
X_train_transformed = preprocessor.fit_transform(X_train)
X_test_transformed = preprocessor.transform(X_test)

# Convert transformed arrays back to DataFrames with appropriate column names
num_feature_names = numerical_columns
cat_feature_names = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_columns)
feature_names = np.concatenate([num_feature_names, cat_feature_names])

X_train = pd.DataFrame(X_train_transformed, columns=feature_names, index=X_train.index)
X_test = pd.DataFrame(X_test_transformed, columns=feature_names, index=X_test.index)

# Step 4: Standardize the features
# Identify categorical and numerical columns
categorical_columns = X.select_dtypes(include=['object']).columns
numerical_columns = X.select_dtypes(include=[np.number]).columns

# Create a ColumnTransformer to apply different preprocessing to different columns
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
# Explicitly import the Pipeline class
from sklearn.pipeline import Pipeline

# Define preprocessors for categorical and numerical features
categorical_preprocessor = Pipeline(steps=[
    ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore')) # Use sparse=False for compatibility with StandardScaler
])

numerical_preprocessor = Pipeline(steps=[
    ('scaler', StandardScaler())
])

# Combine preprocessors into a ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_preprocessor, numerical_columns),
        ('cat', categorical_preprocessor, categorical_columns)
    ])

# Fit and transform the data
X_train_scaled = preprocessor.fit_transform(X_train)
X_test_scaled = preprocessor.transform(X_test)

# Get feature names after transformation
num_feature_names = numerical_columns
cat_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_columns)
feature_names = np.concatenate([num_feature_names, cat_feature_names])

# Convert back to DataFrames with correct column names and indices
X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_names, index=X_train.index)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_names, index=X_test.index)

# Step 5: Define models
models = {
    "Naive Bayes": GaussianNB(),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "SVM": SVC(random_state=42, probability=True),
    "KNN": KNeighborsClassifier()
}

# Step 6: Cross-validation
cv_results = {}
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for name, model in models.items():
    scores = cross_val_score(model, X_train_scaled, y_train, cv=kf, scoring='accuracy')
    cv_results[name] = scores.mean()
    print(f"{name} Cross-validation accuracy: {scores.mean():.4f}")

# Plot cross-validation results
plt.figure(figsize=(8, 5))
plt.bar(cv_results.keys(), cv_results.values())
plt.title('Cross-Validation Accuracy for Each Model')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.show()

# Step 7: Train models and evaluate on test data
classification_reports = {}
test_accuracies = {}

for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    classification_reports[name] = classification_report(y_test, y_pred, output_dict=True)
    test_accuracies[name] = accuracy_score(y_test, y_pred)

# Step 8: Compare precision, recall, F1 score, and accuracy
precision = [classification_reports[name]['weighted avg']['precision'] for name in models]
recall = [classification_reports[name]['weighted avg']['recall'] for name in models]
f1 = [classification_reports[name]['weighted avg']['f1-score'] for name in models]
accuracy = list(test_accuracies.values())

# Plot the comparison
plt.figure(figsize=(12, 6))
bar_width = 0.2
index = np.arange(len(models))

plt.bar(index, precision, bar_width, label='Precision')
plt.bar(index + bar_width, recall, bar_width, label='Recall')
plt.bar(index + 2 * bar_width, f1, bar_width, label='F1 Score')
plt.bar(index + 3 * bar_width, accuracy, bar_width, label='Accuracy')

plt.xlabel('Models')
plt.ylabel('Scores')
plt.title('Model Comparison (Precision, Recall, F1, Accuracy)')
plt.xticks(index + 1.5 * bar_width, models.keys(), rotation=45)
plt.legend()
plt.tight_layout()
plt.show()